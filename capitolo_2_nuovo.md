Capitolo 2: L'Intelligenza del Sistema - Correlazione Geografica e Meteo
 
Con una solida base dati e un'infrastruttura centralizzata operativa, il passo successivo è stato sviluppare i sistemi che arricchiscono i dati grezzi con intelligenza contestuale. Questo capitolo descrive come è stato creato il collegamento tra la composizione geografica del portafoglio clienti e i dati meteorologici, trasformando informazioni statiche in metriche ponderate che riflettono l'effettivo impatto delle condizioni meteo sui consumi energetici.

2.1. Abbinamento POD-Località: La Mappatura Intelligente

Per comprendere come le condizioni meteorologiche influenzano i consumi del portafoglio, è prima necessario sapere dove si trovano fisicamente i punti di prelievo. Questo può sembrare semplice, ma in realtà rappresenta una sfida tecnica non banale quando si gestiscono 60.000 POD distribuiti su centinaia di comuni italiani con un portafoglio che varia ogni mese.
Il sistema di abbinamento POD-località si integra automaticamente con l'output dell'elaborazione RCU descritta nel capitolo precedente. Quando l'elaborazione RCU termina, un file batch aziendale prende il file RCU_INDIRIZZI.xlsx generato, ne crea una copia rinominata report_indirizzi_ggmmaaaa.xlsx nella directory del sistema di abbinamento, e rilevando la presenza del report del mese corrente, avvia automaticamente questo processo. Questa separazione su due script distinti migliora manutentabilità e scalabilità, pur mantenendo l'integrazione tramite batch aziendale.
Il file RCU_INDIRIZZI contiene, per ogni POD, informazioni anagrafiche tra cui l'indirizzo completo con comune, provincia e CAP. Il sistema di abbinamento processa questo file estraendo per ogni POD il nome del comune di appartenenza e normalizzandolo per gestire varianti ortografiche, abbreviazioni e prefissi come "Comune di" o "Città di". Questo processo di normalizzazione è fondamentale perché i nomi dei comuni possono presentarsi in formati diversi tra le varie fonti dati.
Una volta normalizzati i nomi dei comuni, il sistema li abbina alle zone di mercato. Il mercato elettrico italiano è suddiviso in sette zone: Nord, Centro Nord, Centro Sud, Sud, Sicilia, Calabria e Sardegna. Ogni comune italiano appartiene a una di queste zone secondo una mappatura ufficiale. Il sistema mantiene una tabella di riferimento che associa ogni comune alla sua zona di mercato, permettendo di classificare automaticamente ogni POD.
Il risultato di questo processo è una tabella nel database SQL che contiene, per ogni POD in portafoglio, il comune di appartenenza e la zona di mercato. Questa informazione diventa poi la base per tutti i calcoli successivi di incidenza geografica e media ponderata. La tabella viene aggiornata mensilmente quando arrivano i nuovi dati RCU, garantendo che rifletta sempre la composizione corrente del portafoglio, inclusi nuovi clienti acquisiti e clienti che hanno cessato il contratto.

2.2. Calcolo delle Incidenze: Media Ponderata vs Media Semplice

Una volta che ogni POD è stato mappato alla sua località, il passo successivo è calcolare quanto ciascuna località pesa sul consumo totale di ogni zona di mercato. Questo calcolo delle incidenze è il cuore dell'innovazione analitica del sistema.
Il concetto di media ponderata si contrappone a quello di media semplice. Con una media geografica semplice, se vogliamo calcolare la temperatura media della zona Nord, prendiamo tutti i comuni della zona, ne calcoliamo la temperatura media aritmetica, e otteniamo un valore che rappresenta "la temperatura media del Nord". Il problema è che questo approccio tratta allo stesso modo Milano, con cinque milioni di abitanti e migliaia di POD in portafoglio, e un piccolo comune alpino con duemila abitanti e dieci POD ma dal punto di vista dei consumi energetici il loro peso è completamente diverso.
La media ponderata risolve questo problema assegnando a ciascuna località un peso proporzionale alla sua incidenza effettiva sui consumi. Il calcolo avviene in questo modo: il sistema interroga il database SQL nella tabella TIS che contiene i consumi effettivi caricati dall'elaborazione RCU, filtra solo i misuratori di tipo '2G' poiché sono gli unici per cui l'azienda ha deciso di elaborare degli aggiustamenti del modello previsionale di base, aggrega i consumi per comune sommando tutti i conuntivi di tutti i POD presenti in quel comune in un determinato mese, calcola il consumo totale della zona di mercato sommando i consumi di tutti i comuni appartenenti a quella zona, e infine calcola l'incidenza percentuale di ogni comune dividendo il suo consumo per il consumo totale della zona.
Il risultato è una tabella di incidenze che dice, per esempio, che Milano rappresenta il trenta percento dei consumi della zona Nord, Torino il quindici percento, Genova il dieci percento, e così via per tutti i comuni dove abbiamo clienti. Questi pesi vengono salvati nella tabella SQL dedicata EE_LocalitaIncidenzaConsumi e aggiornati mensilmente quando arrivano i nuovi dati di consumo.
La differenza pratica tra media semplice e media ponderata può essere sostanziale. Immaginiamo una giornata estiva dove Milano registra 35 gradi mentre i comuni alpini del Nord hanno 22 gradi. Con una media semplice, se ci sono molti piccoli comuni alpini, la temperatura media della zona potrebbe risultare intorno ai 25-26 gradi. Ma se Milano da sola rappresenta il trenta percento dei consumi della zona, la temperatura "rilevante" per i consumi è molto più alta, probabilmente intorno ai 30-31 gradi. Questa differenza di 4-5 gradi nella metrica meteorologica si traduce in errori significativi nelle previsioni di consumo, perché il modello ML vedrebbe una temperatura media più bassa di quella effettivamente sperimentata dalla maggior parte dei clienti.

2.3. Integrazione Dati Meteo: API e Sincronizzazione Automatica

Con le incidenze calcolate, il sistema può ora acquisire i dati meteorologici e calcolare le medie ponderate. L'integrazione meteo avviene attraverso API Open-Meteo, un servizio che fornisce previsioni meteorologiche a pagamento con buona affidabilità.
Il modulo di integrazione meteo opera su due livelli temporali. Per i dati storici, scarica le osservazioni meteorologiche passate per tutte le località rappresentative del portafoglio. Questo storico serve per l'analisi retrospettiva delle correlazioni tra meteo e consumi e per addestrare i modelli di Machine Learning. Per i dati futuri, scarica le previsioni meteorologiche per i giorni successivi, che sono le informazioni realmente utilizzabili in produzione quando si devono fare forecast operativi.
Per ogni località, il sistema scarica diverse variabili meteorologiche: temperatura, che è la variabile più rilevante per i consumi residenziali e commerciali, irraggiamento solare, che influenza sia la produzione da fonti rinnovabili che l'uso del condizionamento, umidità relativa, che interagisce con la temperatura per determinare la percezione termica effettiva, e condizioni prevalenti come sereno, nuvoloso, pioggia, che possono influenzare i comportamenti di consumo.
Una volta scaricati i dati grezzi per ogni località, il sistema applica le incidenze calcolate nella sezione precedente per calcolare la media ponderata. Per ogni zona di mercato e per ogni ora, la temperatura media ponderata si calcola moltiplicando la temperatura di ogni località per la sua incidenza percentuale e sommando tutti i contributi. Lo stesso processo viene applicato a tutte le altre variabili meteorologiche.
Un aspetto architetturale fondamentale del sistema meteorologico è la sua struttura a tre tabelle distinte, ciascuna con uno scopo specifico. La prima tabella contiene le previsioni meteorologiche per i prossimi 7 giorni, aggiornate quotidianamente tramite API. Questa tabella rappresenta la "finestra sul futuro" utilizzata operativamente: contiene le previsioni più recenti per temperatura, irraggiamento, umidità e altre variabili per i giorni a venire. La seconda tabella contiene lo storico dei dati meteorologici effettivi, cioè le condizioni meteo realmente verificatesi nel passato. Questa tabella viene popolata scaricando periodicamente i dati osservati dalle stazioni meteorologiche e rappresenta la "verità storica" di cosa è realmente accaduto. La terza tabella, più sofisticata, contiene lo storico delle previsioni passate** con i loro timestamp di emissione.
Questa terza tabella merita particolare attenzione perché risolve un problema tecnico critico nell'addestramento dei modelli di Machine Learning. Le API meteorologiche emettono previsioni che vengono costantemente aggiornate man mano che ci si avvicina al giorno di riferimento. Una previsione per dopodomani emessa oggi sarà diversa dalla previsione per lo stesso giorno emessa domani, che a sua volta sarà diversa da quella emessa tra due giorni. Quando si addestra un modello ML, è fondamentale utilizzare esattamente le stesse informazioni che sarebbero state disponibili nel momento in cui si sarebbe dovuta fare la previsione. Se si utilizzassero i dati meteo effettivi o le previsioni più recenti per un giorno del passato, si commetterebbe un errore chiamato "fuga di informazioni": il modello vedrebbe dati che in realtà non erano disponibili in quel momento, imparando pattern irrealistici e producendo performance artificialmente gonfiate che non si replicherebbero in produzione.
La tabella dello storico previsioni risolve questo problema salvando ogni previsione con il suo timestamp di emissione. Al momento il modello ML è stato addestrato con lo storico dei dati meteo, l'obbiettivo sarebbe invece utilizzare esattamente le previsioni meteo che erano disponibili in quel momento, non quelle aggiornate o i dati effettivi. Questo garantisce che l'addestramento rispecchi fedelmente le condizioni operative reali.
Le tre tabelle servono quindi scopi complementari: lo storico delle previsioni quando raggiungerà una buona quantità di dati verrà utilizzato esclusivamente per l'addestramento e validazione dei modelli di Machine Learning, garantendo che imparino da informazioni realisticamente disponibili. Lo storico dei dati effettivi combinato con le previsioni per i prossimi 7 giorni vengono invece utilizzati per la visualizzazione grafica nei pannelli di controllo descritti nel capitolo tre, permettendo agli operatori di vedere sia cosa è accaduto nel passato recente sia cosa ci si aspetta nei giorni futuri, il tutto sulla stessa scala temporale per facilitare l'interpretazione dei trend.

2.4. Il Valore dei Dati Ponderati per il Forecasting



L'intero sistema di medie ponderate potrebbe sembrare un'elaborazione eccessivamente complessa per quello che in fondo è "solo" un calcolo di temperature medie. In realtà, la differenza tra utilizzare medie geografiche semplici e medie ponderate sui consumi effettivi si manifesta direttamente nella qualità delle previsioni e, di conseguenza, nei risultati economici dell'azienda.
Test empirici condotti confrontando modelli addestrati con le due diverse tipologie di metriche hanno evidenziato miglioramenti nell'accuratezza predittiva di diversi punti percentuali. Questo può sembrare un guadagno modesto in termini relativi, ma quando viene applicato a volumi di energia dell'ordine di centinaia di migliaia di megawattora all'anno, si traduce in risparmi economici tangibili sulle operazioni di bilanciamento.
Il valore del sistema va però oltre la pura accuratezza numerica. Le metriche ponderate offrono agli operatori uno strumento di interpretazione immediato: quando la dashboard segnala una temperatura media ponderata prevista di 32 gradi per la zona Nord, quel valore non è una statistica astratta ma rappresenta concretamente le condizioni che sperimenterà la maggior parte del portafoglio clienti. Questa interpretabilità diretta facilita le decisioni operative sulle allocazioni manuali, perché si basa su indicatori che hanno un significato operativo concreto.
Un altro aspetto fondamentale è la resilienza del sistema ai cambiamenti di composizione del portafoglio. L'acquisizione di un nuovo grande cliente industriale o la perdita di contratti significativi non richiedono interventi manuali sul sistema di calcolo: le incidenze vengono automaticamente ricalcolate ogni mese integrando i nuovi dati di consumo, e il peso di ciascuna località nella media ponderata si adatta di conseguenza. Il sistema "impara" automaticamente la nuova geografia economica del portafoglio.
Questo flusso completamente automatizzato si integra perfettamente con il resto dell'ecosistema: i dati fluiscono dalla tabella TIS alle tabelle delle incidenze, dalle API meteo alle tabelle dei dati grezzi, e da lì alle metriche ponderate che alimentano sia i modelli ML che le dashboard operative. Ogni componente lavora sempre con informazioni aggiornate senza richiedere coordinamento manuale.
L'investimento nello sviluppo di questa infrastruttura si giustifica quindi non solo con i benefici immediati in termini di accuratezza, ma anche con la scalabilità e la manutenibilità del sistema nel lungo periodo. Una volta costruito, il meccanismo continua a funzionare e ad adattarsi autonomamente, rappresentando un asset permanente per l'azienda anziché un intervento una tantum.

Conclusioni del Capitolo

Questo capitolo ha descritto come è stato costruito il ponte tra la realtà fisica del portafoglio clienti distribuito geograficamente e le condizioni meteorologiche che influenzano i loro consumi. L'abbinamento POD-località crea la mappa fondamentale, il calcolo delle incidenze trasforma questa mappa in pesi quantitativi, e l'integrazione meteo applica questi pesi per generare metriche aggregate significative.
Questa infrastruttura di dati contestuali rappresenta la base su cui poggia tutto il sistema di forecasting avanzato. Nel prossimo capitolo vedremo come questi dati vengono utilizzati dai modelli di Machine Learning e dalle dashboard interattive per generare previsioni accurate e supportare le decisioni operative quotidiane.


Capitolo 3: Il Cuore del Sistema - Elaborazione Dati e Machine Learning

Con una solida base dati, un'infrastruttura centralizzata operativa, e un sistema di metriche meteorologiche ponderate, è ora possibile affrontare la sfida più complessa e strategicamente rilevante del progetto: re-ingegnerizzare il processo di previsione dei consumi energetici. Questo capitolo descrive come il sistema di forecasting è stato trasformato da un approccio tradizionale deterministico a un ecosistema intelligente che integra Machine Learning, analisi interattiva e supporto decisionale, mantenendo l'operatore al centro del processo.

3.1. Il Modulo Data Sourcing: Orchestratore Centralizzato

Come descritto in dettaglio nella sezione 1.3, il modulo data sourcing rappresenta il nucleo operativo che orchestra l'intero flusso di lavoro del forecasting. Qui riassumiamo brevemente il suo ruolo nel contesto specifico della generazione delle previsioni.
Il modulo prepara i dataset necessari per l'addestramento e l'inferenza dei modelli di Machine Learning gestendo join complesse tra molteplici fonti: i consuntivi storici dalla tabella TIS e dalle tabelle 1G/2G, i dati meteorologici ponderati calcolati dal sistema descritto nel capitolo precedente, le caratteristiche temporali come festività, tipo di giorno, stagionalità, il forecast base tradizionale che serve come punto di partenza per il primo livello ML.
Una volta preparati i dataset, il modulo coordina l'esecuzione sequenziale dei modelli di primo e secondo livello, salva le previsioni generate nelle tabelle SQL dedicate con versionamento appropriato, aggiorna le statistiche aggregate utilizzate dalle dashboard per il calcolo delle performance, e rende disponibili i dati per la fase successiva di generazione ordini o per la visualizzazione interattiva.
Questa orchestrazione centralizzata garantisce che l'intero processo sia ripetibile, tracciabile e completamente automatizzato, eliminando la possibilità di errori dovuti a passaggi manuali dimenticati o eseguiti in ordine errato.

3.2. Il Processo Attuale: Forecasting Manuale e Sincronizzazione Excel-SQL

Prima di descrivere i modelli di Machine Learning, è importante chiarire come attualmente avviene il processo operativo di forecasting e come i nuovi strumenti si integrano con il workflow esistente. Questa comprensione è fondamentale perché il sistema ML non ha sostituito il processo tradizionale, ma lo affianca come strumento di supporto e validazione.
Il forecasting principale avviene ancora tramite file Excel, dove risiede il modello tradizionale basato sulla ricalendarizzazione dei consumi dell'anno precedente con aggiustamenti per la crescita del portafoglio. Questo file è familiare agli operatori che lo utilizzano da anni e incorpora logiche di business consolidate. Ogni giorno, l'operatore apre il file Excel, visualizza il forecast base generato automaticamente dalle macro VBA, e applica correzioni manuali basate sulla sua esperienza, conoscenza del mercato, e intuizione sulle condizioni previste.
L'innovazione introdotta è stata l'integrazione di questo processo con il database SQL centralizzato. Il file Excel è stato potenziato con un pulsante collegato a una macro VBA che, quando premuto, trasferisce automaticamente i dati del forecast modificato manualmente nel database SQL Server in tabelle dedicate. Questa integrazione apparentemente semplice ha un impatto operativo trasformativo.
Nel momento stesso in cui l'operatore applica una modifica nel file Excel e preme il pulsante di trasferimento, queste vengono immediatamente storicizzate nel database con timestamp preciso e identificativo dell'utente. Le allocazioni manuali diventano quindi immediatamente disponibili per l'analisi nelle dashboard interattive che si aggiornano in tempo reale. L'operatore può visualizzare la dashboard in diretta mentre applica le variazioni al forecast base per vedere graficamente il risultato delle sue modifiche sovrapposte ad altre fonti di informazione: le previsioni dei modelli ML che chiamiamo Forecast V1 e Forecast V2, i dati meteorologici previsti, la media mobile calcolata sui consumi delle ultime settimane.
Questo ciclo di feedback immediato trasforma il processo decisionale da un'operazione isolata a un'interazione continua con i dati. Se l'operatore vede che la sua allocazione si discosta significativamente dalla media mobile o dai forecast ML senza una ragione valida, può  correggere la sua variazione, ricaricare nel database, e riverificare nella dashboard. Questo processo iterativo di perfezionamento è ora possibile grazie all'integrazione fluida tra Excel tradizionale e infrastruttura SQL moderna.

3.3. Limiti del Forecasting Tradizionale

Il metodo tradizionale di ricalendarizzazione, per quanto consolidato e familiare, presenta limiti strutturali che ne impediscono il miglioramento oltre una certa soglia di accuratezza.
Il primo limite è l'assenza di considerazione delle variabili meteorologiche in modo sistematico. Un mercoledì di luglio con temperature di 40 gradi genera consumi per il condizionamento completamente diversi da un mercoledì con temperature di 25 gradi, ma il modello tradizionale tratta questi due giorni allo stesso modo se cadono nello stesso punto del calendario. L'operatore può correggere manualmente considerando il meteo, ma questo richiede valutazione soggettiva e non è sistematico.
Il secondo limite è l'incapacità di apprendere dai propri errori. Se il sistema sbaglia sistematicamente le previsioni durante i ponti perché sottostima la riduzione dei consumi industriali, continuerà a ripetere lo stesso errore anno dopo anno. Non c'è un meccanismo di feedback automatico che identifichi pattern ricorrenti negli errori e li corregga.
Il terzo limite è la difficoltà nella gestione di situazioni non standard. Eventi meteorologici estremi, festività mobili che cadono in posizioni diverse del calendario, variazioni strutturali nei comportamenti di consumo: tutte queste situazioni richiedono intervento umano per correggere il forecast base. L'operatore esperto sa come gestirle, ma la conoscenza rimane tacita e non codificata in un sistema riproducibile.

3.4. Livello 1: Error Correction Model

Il cuore del nuovo sistema di forecasting è un modello di Machine Learning strutturato su due livelli sequenziali. Il primo livello, chiamato Error Correction Model, è dedicato alla correzione degli errori sistematici del forecast base tradizionale.
L'intuizione alla base di questo modello è che, pur con tutti i suoi limiti, il forecast tradizionale cattura correttamente il pattern generale dei consumi. Gli errori che commette non sono casuali, ma seguono pattern ricorrenti e prevedibili. Per esempio, tende sistematicamente a sovrastimare i consumi durante i ponti quando molte aziende sono chiuse, o sottostima durante ondate di caldo prolungate quando l'uso del condizionamento aumenta oltre le aspettative storiche.
Il primo livello non cerca quindi di predire direttamente il consumo, operazione che richiederebbe di modellare tutte le complesse dinamiche del mercato energetico. Invece, si concentra esclusivamente sulla predizione dell'errore del forecast base. Questo approccio semplifica enormemente il problema da risolvere. Il modello impara a riconoscere quando il forecast base sta per sbagliare e di quanto, utilizzando come indicatori le caratteristiche temporali del giorno da prevedere.
Le features utilizzate sono state progettate con particolare attenzione alla loro disponibilità in produzione. Includono trasformazioni cicliche di ora, giorno della settimana e mese per catturare la natura periodica del tempo. Una variabile binaria che indica se il giorno è feriale, festivo o ponte. Il nome della festività specifica quando applicabile, perché Natale ha pattern di consumo diversi da Ferragosto. Le medie e statistiche calcolate sui consumi e sugli errori passati, con attenzione a usare finestre temporali che rispettino il problema della disponibilità effettiva dei dati.
Un vincolo cruciale che ha guidato la progettazione riguarda proprio i lag temporali. I dati di consumo effettivo arrivano con ritardo di 48 ore. Quando devi fare una previsione per domani, non hai ancora i dati di consuntivo di ieri. Per evitare data leakage, ovvero, l'uso di informazioni che in produzione non sarebbero disponibili, tutte le features basate su dati storici utilizzano un lag minimo di 48 ore. Il modello può guardare ai consumi di due giorni fa, una settimana fa, ma mai a ieri.
L'algoritmo utilizzato è LightGBM, un'implementazione avanzata di Gradient Boosting Decision Trees particolarmente adatta perché è estremamente veloce, gestisce nativamente variabili categoriche, è robusto ai valori mancanti, e fornisce informazioni sull'importanza delle features. Gli iperparametri vengono ottimizzati automaticamente con Optuna, un framework che esplora sistematicamente lo spazio dei parametri e identifica la configurazione che minimizza l'errore di previsione su un set di validazione temporale.
L'output del primo livello è una previsione corretta ottenuta sottraendo l'errore predetto dal forecast base. Se il forecast tradizionale prevede 1000 MWh e il modello predice un errore di +50 MWh, significa che il forecast base tende a sovrastimare di 50, quindi la previsione corretta sarà 950 MWh. Questa previsione, chiamata Forecast V1, viene generata quotidianamente e storicizzata per valutarne le performance nel tempo.

3.5. Livello 2: Meteo Refinement Model

Il secondo livello, chiamato Meteo Refinement Model, è dedicato all'affinamento della previsione utilizzando le variabili meteorologiche ponderate descritte nel capitolo precedente. Mentre il primo livello si concentra sui pattern temporali e calendari, il secondo livello cattura le relazioni non lineari tra condizioni meteo e consumi energetici.
La temperatura è il fattore meteorologico più importante. Al di sotto di una certa soglia, aumenta il consumo per il riscaldamento; al di sopra di un'altra soglia, aumenta il consumo per il condizionamento. Ma la relazione non è lineare e dipende da molti altri fattori: l'umidità, la temperatura percepita, la durata dell'ondata di caldo o freddo, le abitudini consolidate della popolazione.
Il secondo livello utilizza come input la previsione già corretta dal primo livello, insieme a un set di features meteorologiche avanzate derivate dai dati ponderati. Oltre alla temperatura media ponderata per zona, include l'irraggiamento solare medio ponderato, che influenza sia la produzione da rinnovabili che l'uso del condizionamento, le condizioni prevalenti come sereno, nuvoloso o pioggia. Vengono create features derivate che catturano pattern più complessi: cooling degree hours e heating degree hours calcolati con soglie non lineari, la variazione di temperatura rispetto al giorno precedente, l'interazione tra temperatura e umidità, la differenza tra temperatura massima e minima prevista.
Un aspetto fondamentale è che i dati meteorologici utilizzati al momento sono osservazioni reali e non previsioni metereologiche, questo perchè l'azienda non disponeva di una struttura per salvare e registrare le previsioni meteo in quanto non si era ancora pensato allo sviluppo di un modello di machine leanring interno di questo genere. 
Questa necessità tecnica ha posto una sfida pratica significativa. Mentre per il primo livello erano disponibili anni di storico di consumi effettivi e caratteristiche temporali, per il secondo livello servono anni di previsioni meteorologiche storiche. L'azienda non disponeva di questo storico, quindi è stato implementato un sistema automatico di raccolta che salva quotidianamente nel database le previsioni per i giorni successivi prima che si avverino. Tuttavia, sarà necessario attendere uno-due anni per avere uno storico sufficientemente lungo.
Per non bloccare completamente lo sviluppo, è stato effettuato un primo addestramento del secondo livello utilizzando dati meteorologici storici effettivi disponibili per circa 6 mesi. Questo approccio, pur non essendo ideale metodologicamente, ha permesso di validare l'architettura e verificare che l'approccio tecnico fosse corretto. Le performance di questo modello preliminare non sono ancora ottimali, ma il sistema è pronto per essere riaddestrato completamente una volta raggiunto lo storico necessario di previsioni meteo.
L'output del secondo livello è la previsione finale, chiamata Forecast V2, che rappresenta la migliore stima possibile del consumo considerando sia pattern temporali che condizioni meteorologiche. Anche questa viene generata e storicizzata quotidianamente.

3.6. Dashboard Interattiva: Confronto Multi-Forecast e Validazione Decisionale

Le previsioni dei modelli ML, per quanto accurate, hanno valore limitato se non sono accessibili e comprensibili per gli operatori che devono prendere decisioni concrete. È stata quindi sviluppata una dashboard interattiva che trasforma i dati grezzi in informazioni azionabili.
La dashboard si basa su tecnologie moderne: Dash e Plotly per il frontend interattivo, pandas per l'elaborazione dati, e SQL Server come backend dati. È stata pacchettizzata come applicazione eseguibile standalone che può essere lanciata con un doppio click senza richiedere installazione di Python o librerie.
L'interfaccia è organizzata in tab tematici. Il tab principale di analisi forecast permette di confrontare visivamente le performance dei diversi modelli di previsione. L'utente seleziona un periodo di interesse e una zona di mercato, e il sistema mostra immediatamente un grafico temporale dove sono sovrapposte tutte le serie rilevanti: il consumo effettivo con linea spessa tratteggiata, il forecast base tradizionale, il Forecast V1 del primo livello ML, il Forecast V2 del secondo livello ML, e il forecast manuale inserito dall'operatore tramite Excel.
Sotto il grafico vengono mostrate metriche quantitative di performance per ogni modello: errore medio assoluto percentuale, errore quadratico medio, numero di giorni analizzati. Grafici aggiuntivi mostrano l'andamento dell'errore nel tempo permettendo di identificare pattern sistematici, e la distribuzione statistica dell'errore tramite istogramma.
Il tab meteo e consumi visualizza le correlazioni tra condizioni meteorologiche e consumi attraverso grafici a doppio asse che mostrano simultaneamente consumi e temperatura. Questa visualizzazione rende immediatamente visibili pattern stagionali: nei mesi estivi si vede chiaramente come i consumi aumentino quando le temperature superano una certa soglia per il condizionamento, mentre nei mesi invernali la relazione si inverte.
Una funzionalità particolarmente utile è la possibilità di evidenziare un giorno specifico su tutti i grafici. L'operatore può selezionare il giorno per cui deve decidere le allocazioni, e quel giorno viene evidenziato con un rettangolo trasparente e linee verticali tratteggiate, permettendo di metterlo a fuoco nel contesto dei giorni circostanti.
Il tab correlazioni offre strumenti di analisi statistica avanzati: una matrice di correlazione visualizzata come heatmap mostra le relazioni tra tutte le variabili numeriche, e grafici a dispersione mostrano la relazione non lineare tra temperatura e consumo punto per punto.
La dashboard include un sistema di caching intelligente basato su checksum SQL. Invece di ricaricare tutti i dati ogni volta, il sistema calcola un hash aggregato dei dati rilevanti tramite una query veloce. Se il checksum non è cambiato, i dati vengono presi dalla cache. Se è cambiato, vengono ricaricati solo i dati modificati. Questo rende la dashboard estremamente reattiva pur lavorando con centinaia di migliaia di record.

3.7. La Curva Suggerita: Algoritmo Avanzato per Validare le Allocazioni

L'elemento più innovativo della dashboard è la curva suggerita, un algoritmo che propone all'operatore un'allocazione ottimale basata su pattern storici e condizioni meteorologiche previste.
L'algoritmo opera attraverso una sequenza di passaggi sofisticati. Primo, analizza il giorno di interesse calcolando la temperatura media prevista e identificando la temperatura massima prevista e in quale ora si verificherà. Secondo, ricerca nello storico degli ultimi due mesi giorni con caratteristiche simili, distinguendo tra feriali e weekend. I giorni candidati devono avere temperatura media comparabile, picco di temperatura dello stesso livello e nello stesso momento della giornata. Terzo, per ciascun giorno simile identificato, recupera i consumi effettivi orari registrati e ne calcola la media, ottenendo una curva oraria rappresentativa.
Ma l'algoritmo non si limita a questa proiezione storica. Introduce un aggiustamento intelligente basato sui consumi più recenti. Se i giorni immediatamente precedenti mostrano un andamento sistematicamente diverso rispetto alla media storica, la curva suggerita viene calibrata per tenerne conto. Questo meccanismo permette di adattarsi gradualmente a eventuali cambiamenti strutturali nei consumi senza reagire eccessivamente a fluttuazioni casuali.
Il risultato viene visualizzato nel grafico come linea arancione tratteggiata. Quando l'operatore inserisce le allocazioni manuali tramite Excel e le carica nel database, la dashboard le mostra sovrapposte alla curva suggerita come linea gialla. L'operatore può così validare immediatamente le sue scelte confrontandole con il suggerimento algoritmico.
Questo non significa che l'operatore debba seguire ciecamente la curva suggerita. La decisione finale rimane sempre umana. Ma fornisce un punto di riferimento oggettivo basato sui dati che riduce il rischio di errori grossolani dovuti a dimenticanze o valutazioni affrettate.

**immagine dashboard con curva media suggerita**

3.8. Visione Futura: Da Supporto Decisionale a Trading Strategico

Il sistema descritto in questo capitolo è attualmente in fase di testing operativo. Le previsioni ML vengono generate e storicizzate quotidianamente, ma non sono ancora utilizzate direttamente per gli ordini di mercato. Servono come strumento di supporto e validazione per le allocazioni manuali.
L'obiettivo a medio termine è validare completamente questi modelli attraverso almeno sei mesi di testing forward-looking. Solo se le performance si dimostreranno consistentemente superiori al metodo tradizionale, il sistema potrà essere considerato maturo per un utilizzo diretto delle previsioni automatiche, pur mantenendo sempre supervisione umana.
Ma la visione a lungo termine va oltre il semplice miglioramento dell'accuratezza predittiva. Una volta che il sistema sarà in grado di prevedere i consumi con alta affidabilità, si aprirà la possibilità di strategie di trading più sofisticate. Invece di cercare semplicemente di azzeccare il consumo esatto per minimizzare gli sbilanciamenti, si potrebbe esplorare la possibilità di sbilanciamenti intenzionali e strategici.
L'idea è integrare un modulo di analisi dei prezzi di mercato che confronti il Prezzo Unico Nazionale con i prezzi previsti di sbilanciamento. Quando il modello prevede che il prezzo di sbilancio sarà superiore al PUN, si potrebbe sovra-allocare intenzionalmente, vendendo implicitamente energia al prezzo di sbilancio più alto. Quando invece il prezzo di sbilancio sarà inferiore, si potrebbe sotto-allocare, acquistando la differenza al prezzo più basso. Questa evoluzione da forecasting accurato a trading strategico rappresenterebbe un cambio di paradigma da cost-reduction a profit-generation.
Naturalmente, questa strategia richiede non solo capacità predittive affidabili sui consumi, ma anche modelli accurati dei prezzi di sbilanciamento, comprensione profonda delle dinamiche di mercato, e gestione attenta del rischio. Ma rappresenta il naturale sviluppo futuro di un sistema che ha dimostrato di poter prevedere accuratamente i consumi.

Conclusioni del Capitolo

Questo capitolo ha descritto come il cuore del sistema, il processo di forecasting, sia stato re-ingegnerizzato attraverso l'introduzione del Machine Learning, pur mantenendo un approccio graduale e prudente. L'architettura a due livelli separa la correzione degli errori calendari dall'affinamento meteorologico, creando modularità e interpretabilità.
L'integrazione con il processo manuale esistente attraverso il ponte Excel-SQL e le dashboard interattive permette agli operatori di continuare a lavorare con strumenti familiari mentre beneficiano di analisi avanzate e suggerimenti algoritmici. La curva suggerita trasforma il processo decisionale da attività basata principalmente sull'intuito a operazione guidata dai dati.
Nel prossimo capitolo vedremo come questo ecosistema di forecasting si completa con i sistemi di generazione e verifica automatica degli ordini di mercato, chiudendo il ciclo operativo dall'acquisizione dati fino all'invio e controllo degli ordini al GME.

